Scrapy - very fast
Made using twisted, an event driven networking framework
Has asynchronize capabilities - doesn't wait for response when dealing with multiple requests
Portable - requires no dependencies unlike beautiful soup - works on every operating system

Spider - define how a site should be scraped, what information will be got, etc. - defined as classes

Selectors - mechanisma for selecting data
eg. links = response.xpath('//a/@href')
divs = response.xpath('//div')
paragraphs = response.xpath('//p/')

Items = data extracted from the selectors - 'containers' for data
Defines the structure of scrapped data and fields it should have

Itemloaders = 

(((((Activating virtual environment: midtermPro\Scripts\Activate)))))
(((((((((python.exe -m pip install --upgrade pip))))))))) - updating pip
->> scrapy startproject projectname
->>scrapy genspider allrecipes 
### Usage
=====
  scrapy genspider [options] <name> <domain>

Generate new spider using pre-defined templates

Options
=======
  -h, --help            show this help message and exit
  -l, --list            List available templates
  -e, --edit            Edit spider after creating it
  -d TEMPLATE, --dump TEMPLATE
                        Dump template to standard output
  -t TEMPLATE, --template TEMPLATE
                        Uses a custom template.
  --force               If the spider already exists, overwrite it with the template

Global Options
--------------
  --logfile FILE        log file. if omitted stderr will be used
  -L LEVEL, --loglevel LEVEL
                        log level (default: DEBUG)
  --nolog               disable logging completely
  --profile FILE        write python cProfile stats to FILE
  --pidfile FILE        write process ID to FILE
  -s NAME=VALUE, --set NAME=VALUE
                        set/override setting (may be repeated)
  --pdb                 enable pdb on failure

->> scrapy
### Scrapy 2.11.0 - active project: recipes

Usage:
  scrapy <command> [options] [args]

Available commands:
  bench         Run quick benchmark test
  check         Check spider contracts
  crawl         Run a spider
  edit          Edit spider
  fetch         Fetch a URL using the Scrapy downloader
  genspider     Generate new spider using pre-defined templates
  list          List available spiders
  parse         Parse URL (using its spider) and print the results
  runspider     Run a self-contained spider (without creating a project)
  settings      Get settings values
  shell         Interactive scraping console
  startproject  Create new project
  version       Print Scrapy version
  view          Open URL in browser, as seen by Scrapy

Use "scrapy <command> -h" to see more info about a command


->> #Running spider - recipes scrapy crawl allrecipes -o allrecipes.csv(.json)) (o -for output, capital o (O) overides what was previously saved in the json/csv files)


Using pipelines in web scraping with spiders typically refers to the process of cleaning, processing, and storing the scraped data. In the context of scrapy, a popular Python library for web scraping, pipelines are used to define reusable components that process items as they are being scraped.
After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially.

Each item pipeline component (sometimes referred as just “Item Pipeline”) is a Python class that implements a simple method. They receive an item and perform an action over it, also deciding if the item should continue through the pipeline or be dropped and no longer processed.

Typical uses of item pipelines are:

cleansing HTML data

validating scraped data (checking that the items contain certain fields)

checking for duplicates (and dropping them)

storing the scraped item in a database

RUN A SPIDER PROJECT
scrapy crawl myspider




RUNNING MULTIPLE SPIDERS:


You can add the above script to a separate Python file within your Scrapy project directory. This file will serve as the entry point for running multiple spiders. Here's a suggested directory structure for your Scrapy project:

markdown
Copy code
myproject/
    scrapy.cfg
    myproject/
        __init__.py
        items.py
        middlewares.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            allrecipes.py
            afrifood_network.py
            weeatatlast.py
    run_spiders.py
You can name the file where you'll add the script, for example, run_spiders.py. This file will be located at the same level as your Scrapy project settings (scrapy.cfg) and the myproject directory.

Here's how you can structure run_spiders.py:

python
Copy code
from scrapy.crawler import CrawlerProcess
from myproject.spiders import AllrecipesSpider, AfrifoodNetworkSpider, weeatatlastRecipeSpider
from myproject import settings

# Create a list of spider classes
spiders = [AllrecipesSpider, AfrifoodNetworkSpider, weeatatlastRecipeSpider]

# Create a CrawlerProcess with project settings
process = CrawlerProcess(settings=settings)

# Start each spider in the list
for spider in spiders:
    process.crawl(spider)

# Run the process (blocking call)
process.start()
Ensure that you replace myproject with the actual name of your Scrapy project in the import statements and adjust the spider class names if necessary.

To run multiple spiders, navigate to your project directory (myproject) in the terminal and execute the following command:

Copy code
python run_spiders.py

Scraping with JavaScript rendering capabilities - Scrapy Selenium, Playwright, Splash
Yes, you can, thanks to the scrapy-selenium Python package. This Scrapy middleware allows spiders to load pages in Selenium, adding JavaScript rendering capabilities to the scraping tool.
The main problem with this solution is that the middleware hasn't been maintained since 2020. This means it still relies on Selenium 3 and no longer works with the most recent version of browsers, so you might prefer to choose Scrapy Splash or Scrapy Playwright.
Using Selenium with Scrapy provides two main advantages: it enables Scrapy to render JavaScript, as well as allows you to mimic human behavior via browser interaction to avoid getting detected as a bot.
The main pros of Selenium are its vast community, extensive documentation, and cross-language compatibility. This makes it more popular than other headless browsers for browser automation in both testing and scraping.
Using scrapy-Playwright
Start by installing - pip install scrapy-playwright
Install only a subset of the available browsers:playwright install firefox chromium

Change/Add the following in the scrapy settings file: 
Download handler
Replace the default http and/or https Download Handlers through DOWNLOAD_HANDLERS:
# settings.py
DOWNLOAD_HANDLERS = {
    "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
    "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
}
 settings.py
TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"